{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"PrAIvateSearch Own your AI, search the web with it\ud83c\udf10\ud83d\ude0e"},{"location":"#about-praivatesearch","title":"About PrAIvateSearch","text":"<p>PrAIvateSearch is a NextJS web application that aims to implement similar features to SearchGPT, but in an open-source, local and private way. </p>"},{"location":"#flowchart","title":"Flowchart","text":"<p>Flowchart for PrAIvateSearch</p> <p>The process of creating and the functioning of PrAIvateSearch is explained in in the Explanation page.</p>"},{"location":"#installation-and-usage","title":"Installation and usage","text":"<p>For this section, you should have <code>conda</code> package manager, <code>docker</code> and <code>docker compose</code>.</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/AstraBert/PrAIvateSearch.git\ncd PrAIvateSearch\n</code></pre> <ol> <li>Move <code>.env.example</code> to <code>.env</code>...</li> </ol> <pre><code>mv .env.example .env\n</code></pre> <p>...and specify PostgreSQL related variables:</p> <pre><code># .env file\npgql_db=\"postgres\"\npgql_user=\"localhost\"\npgql_psw=\"admin\"\n</code></pre> <ol> <li>Install necessary dependencies with <code>conda</code>:</li> </ol> <pre><code>conda env create -f conda_environment.yaml\n</code></pre> <ol> <li>Set up Crawl4AI inside the <code>conda</code> environment:</li> </ol> <pre><code>conda activate praivatesearch\ncrawl4ai-setup\ncrawl4ai-doctor\nconda deactivate\n</code></pre> <ol> <li>Start third-party services:</li> </ol> <pre><code>docker compose up -d\n</code></pre> <ol> <li> <p>Run <code>Qwen-2.5-1.5B-Instruct</code> on API with FastAPI/Uvicorn:</p> </li> <li> <p>Run <code>Qwen-2.5-1.5B-Instruct</code> on API with FastAPI/Uvicorn:</p> </li> </ol> <p><pre><code>conda activate praivatesearch\ncd qwen-on-api/\nuvicorn main:app --host 0.0.0.0 --port 8000\n</code></pre> You can access the application from <code>http://localhost:3000</code> and chat with it!</p>"},{"location":"#usage-note","title":"Usage note","text":"<p>The NextJS application was successfully developed and tested on a Ubuntu 22.04.3 machine, with 32GB RAM, 22 cores CPU and Nvidia GEFORCE RTX4050 GPU (6GB, cuda version 12.3), python version 3.11.11 (packaged by conda 24.11.0)</p> <p>Although being at a good stage of development, the application is a <code>beta</code> and might still contain bugs and have OS/hardware/python version incompatibilities.</p>"},{"location":"#contributions","title":"Contributions","text":"<p>Contributions are more than welcome! See contribution guidelines for more information :)</p>"},{"location":"#funding","title":"Funding","text":"<p>If you found this project useful, please consider to fund it and make it grow: let's support open-source together!\ud83d\ude0a</p>"},{"location":"#license-and-rights-of-usage","title":"License and rights of usage","text":"<p>This project is provided under MIT license: it will always be open-source and free to use.</p> <p>If you use this project, please cite the author: Clelia (Astra) Bertelli</p>"},{"location":"about/","title":"About PrAIvateSearch","text":"<p>PrAIvateSearch (<code>v2.0-beta.0</code>) is a privacy-first, AI-powered, user-centered and data-safe application aimed at providing a local and open-source alternative to big AI search engines such as SearchGPT or Perplexity AI.</p>"},{"location":"contributing/","title":"Contributing to PrAIvateSearch","text":"<p>Do you want to contribute to this project? Make sure to read this guidelines first :)</p>"},{"location":"contributing/#issue","title":"Issue","text":"<p>When to do it:</p> <ul> <li>You found bugs but you don't know how to solve them or don't have time/will to do the solve</li> <li>You want new features but you don't know how to implement them or don't have time/will to do the implementation</li> </ul> <p>\u26a0\ufe0f Always check open and closed issues before you submit yours to avoid duplicates</p> <p>How to do it:</p> <ul> <li>Open an issue</li> <li>Give the issue a meaningful title (short but effective problem description)</li> <li>Describe the problem following the issue template</li> </ul>"},{"location":"contributing/#traditional-contribution","title":"Traditional contribution","text":"<p>When to do it:</p> <ul> <li>You found bugs and corrected them</li> <li>You optimized/improved the code</li> <li>You added new features that you think could be useful to others</li> </ul> <p>How to do it:</p> <ol> <li>Fork this repository</li> <li>Commit your changes</li> <li>Submit pull request (make sure to provide a thorough description of the changes)</li> </ol>"},{"location":"contributing/#showcase-your-praivatesearch","title":"Showcase your PrAIvateSearch","text":"<p>When to do it:</p> <ul> <li>You modified the base application with new features but you don't want/can't merge them with the original PrAIvateSearch</li> </ul> <p>How to do it:</p> <ul> <li>Go to GitHub Discussions &gt; Show and tell page</li> <li>Open a new discussion there, describing your PrAIvateSearch application</li> </ul>"},{"location":"contributing/#thanks-for-contributing","title":"Thanks for contributing!","text":""},{"location":"explanation/","title":"Search the Web with AI","text":""},{"location":"explanation/#1-introduction","title":"1. Introduction","text":"<p>In the last blog post we introduced PrAIvateSearch <code>v1.0-beta.0</code>, a data-safe and private AI-powered search engine: despite being a fully functional solution, there were still hassles that made it under-perform, specifically:</p> <ul> <li>Content scraping from the URLs resulting from the web search was unreliable and sometimes did not produce correct results.</li> <li>We extracted a series of key-words from web-based contents: keywords were then formatted into JSON and injected into the user prompt as context. This, despite being a viable solution to lighten the prompt, only gave the LLM a partial contextual information.</li> <li>We used RAG for context augmentation, but it often proved inefficient as the retrieved context was passed directly into the user's prompt, making inference computationally intense and time-requiring.</li> <li>Our interface was basic and essential, lacking the dynamic and modern flows of other web applications.</li> <li>The use of Google Search API to search the web was not optimal under a privacy-wise point of view</li> </ul> <p>In this sense, we decided to apply a way simpler and direct workflow, synthesized in the following image:  </p> <p></p>"},{"location":"explanation/#2-building-blocks","title":"2. Building Blocks","text":"<p>The application is made up by three important building blocks:</p> <ul> <li>A dynamic, chat-like user interface built with NextJS \u00a0(launched via docker compose)</li> <li>Third-party local database services: Postgres and Qdrant (launched via docker compose)</li> <li>An API service, which is built with FastAPI and served with Uvicorn \u00a0(launched in an isolated conda environment): this service is responsible for connecting the frontend with the backend third party services, web search and all the functions related to finding an answer to the user's query.</li> </ul> <p>We'll go through all these building blocks, but first let's have a look on what are the pre-requirements and the first steps to get the code to build PrAIvateSearch <code>v2.0-beta.0</code>.</p>"},{"location":"explanation/#2a-first-steps","title":"2a. First steps","text":"<p>To get the necessary code and build the environment to run it, you will need:</p> <ul> <li><code>git</code> toolset to get the code from the GitHub repository</li> <li><code>conda</code> package manager to build the environment from which the API will be launched</li> <li><code>docker</code> and <code>docker compose</code> to serve the UI and the third-party local databases.</li> </ul> <p>Now, to get the code, you can simply run:</p> <pre><code>git clone https://github.com/AstraBert/PrAIvateSearch.git\ncd PrAIvateSearch\n</code></pre> <p>First of all move <code>.env.example</code> to <code>.env</code>...</p> <pre><code>mv .env.example .env\n</code></pre> <p>...and specify PostgreSQL related variables:</p> <pre><code># .env file\npgql_db=\"postgres\"\npgql_user=\"localhost\"\npgql_psw=\"\n</code></pre> <p>You will then need to build the API environment with:</p> <pre><code># create the environment\nconda env create -f conda_environment.yaml\n\n# activate the environment\nconda activate praivatesearch\n\n# run crawl4ai post-installation setup\ncrawl4ai-setup\n\n# run crawl4ai health checks\ncrawl4ai-doctor\n\n# deactivate the environment\nconda deactivate\n</code></pre> <p>And, finally, you will be able to launch the frontend and the databases with:</p> <pre><code># The use of the -d option is not mandatory\ndocker compose up [-d]\n</code></pre>"},{"location":"explanation/#3-user-interface","title":"3. User Interface","text":"<p>The user interface is now based on a NextJS implementation of a modern, dynamic chat interface. The interface is inspired to ChatGPT, and aims at giving the user a similar experience to that of using OpenAI's product.</p> <p>Whenever the user interacts with the UI by sending a message, the NextJS app backend sends a <code>GET</code> request to <code>http://localhost:8000/messages/</code>, where our FastAPI/Uvicorn managed API runs (see below). Once the request is fulfilled, the applications displays the message it got back, which is the answer to the user's query.</p> <p>The NextJS application runs on <code>http://localhost:3000</code> and is launched through docker compose.</p>"},{"location":"explanation/#4-database-services","title":"4. Database Services","text":"<p>Database services are run locally thanks to docker compose: they are completely user-managed and can be easily monitored. This gives the user complete control over the data flow inside the application.</p>"},{"location":"explanation/#4a-qdrant","title":"4a. Qdrant","text":"<p>Qdrant is a vector database service that plays a core role inside PrAIvateSearch. Thanks to Qdrant, we:  </p> <ul> <li>Create and manage a semantic cache, in which we store questions that the user already asked and answers that the LLM gave to them, so that we can use the same answer if the user inputs the same question or a similar one. We use semantic search to find similar questions and use a cut-off threshold of 75% similarity to filter out non-relevant hits: that's why our cache is not just a cache, but it is also semantic.</li> <li>Store the content scraped from the web during the web searching and crawling step: we use a sparse collection for this purpose. We then perform sparse retrieval with Splade, loaded with FastEmbed (a python library managed by Qdrant itself): this first retrieval step yields the 5 top hits for the user's prompt inside the database of contents scraped from the web.</li> </ul> <p>Qdrant is accessible for monitoring, thanks to a complete and easy-to-use interface, on <code>http://localhost:6333/dashboard</code></p>"},{"location":"explanation/#4b-postgres","title":"4b. Postgres","text":"<p>Postgres is a relational database, and it is employed here essentially as a manager for chat memory.  </p> <p>Thanks to a SQLAlchemy-based custom client, we can load all the messages sent during the conversation in the <code>messages</code> table: this table is structured in such a way that it is compatible with the chat template that we set for our LLM, and so the retrieval of the chat history at inference time already provides the language model with the full number of previously sent messages.</p> <p>Postgres can be easily monitored through Adminer, a simple and intuitive database management tool: you just need to provide the database type (PostgreSQL), the database name, the username and the password (variables that are passed to Docker and that you can define inside your <code>.env</code> file) . Also Adminer runs locally and is served through docker compose: it is accessible at <code>http://localhost:8080</code>.</p>"},{"location":"explanation/#5-api-services","title":"5. API Services","text":"<p>As we said, we built a local API service thanks to FastAPI and Uvicorn, two intuitive and easy-to-use tools that make API development seamless.</p> <p>The API endpoint is up at <code>http://localhost:8000/messages/</code> and receives the user messages from the NextJS application. The workflow from receiving a message to returning a response is straightforward:</p> <ol> <li>We check if the user's prompt corresponds to an already asked question thanks to the semantic cache: this cache is basically a dense Qdrant collection, with 768-dimensional vectors produced by LaBSE. If there is a significant match within the semantic cache, we return the same answer used for that match, otherwise we proceed with handling the request</li> <li>If the user's query did not yield a significant match from the semantic cache, we proceed to extracting keywords from the user's natural language prompt with the RAKE (Rapid Automatic Keyword Extraction) algorithm. These keywords will be used to search the web</li> <li>We search the web through DuckDuckGo Search API: using DuckDuckGo ensures more privacy in surfing the net than exploiting Google. This is part of the effort that PrAIvateSearch makes to ensure that the user's data are safe and not indexed by Big Techs for secondary (not-always-so-transparent) purposes.</li> <li>We scrape content using the URLs returned by the web search: to do this, we use Crawl4AI asynchronous web crawler, and we return all the scraped content in markdown format</li> <li>We split the markdown texts from the previous step with LangChain <code>MarkdownTextSplitter</code>: after this step, we will have 1000-charachters long text chunks</li> <li>The chunks are encoded into sparse vectors with Splade and uploaded to a Qdrant sparse collection</li> <li>We  search the sparse collection with the user's original query and retrieve the top 5 most relevant documents</li> <li>These 5 relevant documents are then re-ranked: we employ LaBSE to encode the relevant documents into dense vectors and we evaluate their cosine similarity with the vectorized user's prompt. The most similar document gets retrieved as a context</li> <li>The context is passed as a user's message, and the prompt is passed right after it as a user's message too. In this sense, the LLM only has to reply to the user's prompt, but can access, through chat memory, the web-based context for it</li> <li>Qwen-2.5-1.5B-Instruct performs inference on the user's prompt and generates an answer to it: this answer is then returned as the API final response, and will be displayed in the UI.</li> </ol>"},{"location":"explanation/#6-usage","title":"6. Usage","text":""},{"location":"explanation/#6a-usage-note","title":"6a. Usage note","text":"<p>The NextJS application was successfully developed and tested on a Ubuntu 22.04.3 machine, with 32GB RAM, 22 cores CPU and Nvidia GEFORCE RTX4050 GPU (6GB, cuda version 12.3), python version 3.11.11 (packaged by conda 24.11.0)</p> <p>Although being at a good stage of development, the application is a <code>beta</code> and might still contain bugs and have OS/hardware/python version incompatibilities.</p>"},{"location":"explanation/#6b-getting-praivatesearch-up-and-running","title":"6b. Getting PrAIvateSearch Up and Running","text":"<p>To get PrAIvateSearch up and running you need to have already executed the first steps</p> <p>Once we have launched (from within the PrAIVateSearch folder) the databases backend services and the frontend applications via docker compose with this command :</p> <pre><code>docker compose up\n</code></pre> <p>We can head over to the conda environment we set up in the first steps, and launch the API:</p> <pre><code># activate the environment\nconda activate praivatesearch\n\n# head over to the folder where the app is stored\ncd qwen-on-api/\n\n# launch the application\nuvicorn main:app --host 0.0.0.0 --port 8000\n</code></pre> <p>After loading the several AI models involved in the development of the application, you will see that the API is up and ready to receive requests.</p> <p>If you want to test the API prior to sending requests through the frontend, you can simply use this <code>curl</code> command:</p> <pre><code>curl \"http://0.0.0.0:8000/messages/What%20is%20the%20capital%20of%20France\"\n</code></pre> <p>If everything went smoothly, you should receive a response like this:</p> <pre><code>{\"response\": \"The capital of France is **Paris**.\"}\n</code></pre> <p>And that's all! Now head over to <code>http://localhost:3000</code> and start playing around with PrAIvateSearch!\ud83e\udebf</p>"},{"location":"explanation/#7-conclusion","title":"7. Conclusion","text":"<p>The aim behind PrAIvateSearch is to provide an open-source, private and data-safe alternative to Big Tech solutions. The application is still a beta, so, although its workflow may seem solid, there may still be hiccups, untackled errors and imprecisions. If you want to contribute to the project, report issues and help developing the OSS AI community and environment, feel free to do so on GitHub and to help it with funding.</p> <p>Thanks!\ud83e\udd17</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#pip-dependencies-not-installed","title":"<code>pip</code> dependencies not installed","text":"<p>You could run into an issue which is similar to this one: it will seem that the pip dependencies were not installed. Make sure to:</p> <ol> <li>Install the latest version of <code>conda</code>:</li> </ol> <pre><code>curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n# make sure to run `source ~/.bashrc` if your Linux distro requires it!\n</code></pre> <ol> <li>Remove the <code>praivatesearch</code> environment and re-build it following the suggested installation:</li> </ol> <pre><code>conda env remove -n praivatesearch\nconda env create -f conda_environment.yaml\n</code></pre> <ol> <li>Activate the conda environment and make sure that the activation is smooth:</li> </ol> <pre><code>conda activate praivatesearch\n</code></pre> <p>You should now be all set and ready to launch the application!\ud83c\udf89</p>"}]}